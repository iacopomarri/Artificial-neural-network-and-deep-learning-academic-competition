{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom itertools import chain\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.models import Model, load_model\nfrom skimage.transform import resize\nfrom tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom skimage.morphology import label\nfrom keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\nfrom skimage.io import imread, imshow, concatenate_images\nfrom keras.layers.merge import concatenate, add\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tqdm import tqdm_notebook, tnrange\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Set the seed for random operations. \n# This let our experiments to be reproducible. \nSEED = 1234\ntf.random.set_seed(SEED)  \n\n# Get current working directory\ncwd = os.getcwd()\n\n# Set GPU memory growth\n# Allows to only as much GPU memory as needed\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#JSON file for validation split, create CSV for results, encode mask, IoU metric\n\n# dictionary with the format shown in the Evaluation tab\ndef create_json():\n    training = os.listdir(os.path.join(destination_training, 'images', 'img'))\n    validation = os.listdir(os.path.join(destination_validation, 'images', 'img'))\n\n    dataset_split = {'training': training, 'validation': validation}   \n    with open('dataset_split.json', 'w') as fp:\n          json.dump(dataset_split, fp)\n        \n        \n\n        \n        \n#def dice_loss(y_true, y_pred):\n #   numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n  #  denominator = tf.reduce_sum(y_true + y_pred, axis=(1,2,3))\n   # return 1 - numerator / denominator\n    \ndef combinated_loss(y_true, y_pred):\n    alpha = 0.5\n    return alpha*dice_loss(y_true, y_pred) + (1-alpha)*tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    \n    \n\n\n#IoU Metric\ndef my_IoU(y_true, y_pred):\n    # from pobability to predicted class {0, 1}\n    y_pred = tf.cast(y_pred > 0.5, tf.int32) # when using sigmoid. Use argmax for softmax\n\n    # A and B\n    intersection = tf.reduce_sum(y_true * y_pred)\n    # A or B\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    # IoU\n    return intersection / union\n\n\n#Encode mask for CSV file\ndef rle_encode(img):\n   \n    img = np.round(np.squeeze(img)).astype(np.float32)\n  \n    #Flatten column-wise\n    pixels = img.T.flatten()\n   # pixels = pixels[:-65536]   #provo a togliere la metà dei pixel che viene tutta 1\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\n\n\n\n#Create CSV\ndef create_csv(results, results_dir='./'):\n    \n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    with open(os.path.join('./', csv_fname), 'w') as f:\n        f.write('ImageId,EncodedPixels,Width,Height\\n')\n        count = 0\n        for key, value in results.items():\n            #f.write(key + ',' + str(value) + '\\n')\n            f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set some parameters\nim_width = 256\nim_height = 256\nborder = 5\n#path_train = '../input/train/'\n#path_test = '../input/test/'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ImageDataGenerator\n# ------------------\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\napply_data_augmentation = False\n\n# Create training ImageDataGenerator object\n# We need two different generators for images and corresponding masks\nif apply_data_augmentation:\n    train_img_data_gen = ImageDataGenerator(rotation_range=10,\n                                            width_shift_range=10,\n                                            height_shift_range=10,\n                                            zoom_range=0.3,\n                                            horizontal_flip=True,\n                                            vertical_flip=True,\n                                            fill_mode='constant',\n                                            cval=0,\n                                            rescale=1./255)\n    train_mask_data_gen = ImageDataGenerator(rotation_range=10,\n                                             width_shift_range=10,\n                                             height_shift_range=10,\n                                             zoom_range=0.3,\n                                             horizontal_flip=True,\n                                             vertical_flip=True,\n                                             fill_mode='constant',\n                                             rescale=1./255,\n                                             cval=0)\nelse:\n    train_img_data_gen = ImageDataGenerator(rescale=1./255)\n    train_mask_data_gen = ImageDataGenerator(rescale=1./255)\n\n# Create validation and test ImageDataGenerator objects\nvalid_img_data_gen = ImageDataGenerator(rescale=1./255)\nvalid_mask_data_gen = ImageDataGenerator(rescale=1./255)\ntest_img_data_gen = ImageDataGenerator(rescale=1./255)\ntest_mask_data_gen = ImageDataGenerator(rescale=1./255)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create working directory. Splitting training and validation\n\nvalidation_split = 0.2\n\n#Directories\nsource_dir = '/kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset'\ndestination_dir = '/kaggle/working/dataset'\ndestination_training = os.path.join(destination_dir, 'training')\ndestination_validation = os.path.join(destination_dir, 'validation')\ndestination_test = os.path.join(destination_dir, 'test')\n\npath_train = destination_training\npath_test = destination_test\n\n\n#Se nella directory di destinazione non ci sono le cartelle dataset, training, validation e test, le creo.\n#Se ci sono già, le elimino e le ricreo (per rifare ogni volta il validation diverso, penso)\n\nprint(\"Creating main directories..\")\nif os.path.exists(destination_dir):\n    shutil.rmtree(destination_dir)\nif not os.path.exists(destination_dir):\n    os.mkdir(destination_dir)\nif not os.path.exists(destination_training):\n    os.mkdir(destination_training)\nif not os.path.exists(destination_validation):\n    os.mkdir(destination_validation)\nif not os.path.exists(destination_test):\n    os.mkdir(destination_test)\n    \n#Create images/img and masks/img folder into training and validation directories. Create img into test directory\n\nprint(\"Creating subdirectories..\")\nif not os.path.exists(os.path.join(destination_training, 'images')):\n    os.mkdir(os.path.join(destination_training, 'images'))\nif not os.path.exists(os.path.join(destination_training, 'masks')):\n    os.mkdir(os.path.join(destination_training, 'masks'))\n    \nif not os.path.exists(os.path.join(destination_training, 'images/img')):\n    os.mkdir(os.path.join(destination_training, 'images/img'))\nif not os.path.exists(os.path.join(destination_training, 'masks/img')):\n    os.mkdir(os.path.join(destination_training, 'masks/img'))\n    \n    \nif not os.path.exists(os.path.join(destination_validation, 'images')):\n    os.mkdir(os.path.join(destination_validation, 'images'))\nif not os.path.exists(os.path.join(destination_validation, 'masks')):\n    os.mkdir(os.path.join(destination_validation, 'masks'))\n    \nif not os.path.exists(os.path.join(destination_validation, 'images/img')):\n    os.mkdir(os.path.join(destination_validation, 'images/img'))\nif not os.path.exists(os.path.join(destination_validation, 'masks/img')):\n    os.mkdir(os.path.join(destination_validation, 'masks/img'))\n    \nif not os.path.exists(os.path.join(destination_test, 'images')):\n    os.mkdir(os.path.join(destination_test, 'images'))\nif not os.path.exists(os.path.join(destination_test, 'images/img')):\n    os.mkdir(os.path.join(destination_test, 'images/img'))\n\n\n\n#Split training_set e validation_set, spostandoli dalla directory di input a quella di lavoro\n\n\n#Splitting training and  5936 1531-->1528\nprint(\"Splitting train from input into Training and Validation inside working directory...\")\n\n#messi due numeri divisibili per 8. 7 immagini su 7647 vengono però scartate (a random)\nvalidation_size = 1528 #int(7647 * validation_split)\ntrain_size = 6112 #5936 #7467 - validation_size\n\nfiles = os.listdir(source_dir + '/training/images/img')\nrandom.shuffle(files)\nfiles = iter(files)\ni=0\n\nwhile i < validation_size:\n    file=str(next(files))\n    result = shutil.copy(source_dir + '/training/images/img' + '/' + file, destination_validation + '/images/img')\n    result = shutil.copy(source_dir + '/training/masks/img' + '/' + file, destination_validation + '/masks/img')\n    i += 1\n\nwhile i < validation_size + train_size:\n    file=str(next(files))\n    result = shutil.copy(source_dir + '/training/images/img' + '/' + file, destination_training + '/images/img')\n    result = shutil.copy(source_dir + '/training/masks/img' + '/' + file, destination_training + '/masks/img')\n    i += 1\n    \n\n        \n\n#Copio i file del test dalla directory di input a quella di lavoro\n\nprint(\"Copying test set from input inside working directory...\")\nfor file in os.listdir(source_dir + '/test/images/img'):\n        result = shutil.copy(source_dir + '/test/images/img' + '/' + file, destination_test + '/images/img')\n       \nprint(\"creating JSON file..\")\ncreate_json()\n\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create generators to read images from dataset directory\n# -------------------------------------------------------\n\n# Batch size\nbs = 4\n\n# img shape\nimg_h = 256\nimg_w = 256\n\nnum_classes=2\n\n# Training\n# Two different generators for images and masks\n# ATTENTION: here the seed is important!! We have to give the same SEED to both the generator\n# to apply the same transformations/shuffling to images and corresponding masks\n\ntrain_img_gen = train_img_data_gen.flow_from_directory(os.path.join(destination_training, 'images'),\n                                                       target_size=(img_h, img_w),\n                                                       batch_size=bs, \n                                                       class_mode=None, # Because we have no class subfolders in this case\n                                                       shuffle=True,\n                                                       interpolation='bilinear',\n                                                       seed=SEED)  \ntrain_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(destination_training, 'masks'),\n                                                         target_size=(img_h, img_w),\n                                                         batch_size=bs,\n                                                         class_mode=None, # Because we have no class subfolders in this case\n                                                         shuffle=True,\n                                                         interpolation='bilinear',\n                                                         color_mode='grayscale',\n                                                         seed=SEED)\ntrain_gen = zip(train_img_gen, train_mask_gen)  #Iteratore che mappa l'i-esimo elemento di train_img_gen con l i-esimo di train_mask_gen\n\n# Validation\nvalid_img_gen = valid_img_data_gen.flow_from_directory(os.path.join(destination_validation, 'images'),\n                                                       target_size=(img_h, img_w),\n                                                       batch_size=bs, \n                                                       class_mode=None, # Because we have no class subfolders in this case\n                                                       shuffle=False,\n                                                       interpolation='bilinear',\n                                                       seed=SEED)\nvalid_mask_gen = valid_mask_data_gen.flow_from_directory(os.path.join(destination_validation, 'masks'),\n                                                         target_size=(img_h, img_w),\n                                                         batch_size=bs, \n                                                         class_mode=None, # Because we have no class subfolders in this case\n                                                         shuffle=False,\n                                                         interpolation='bilinear',\n                                                         color_mode='grayscale',\n                                                         seed=SEED)\nvalid_gen = zip(valid_img_gen, valid_mask_gen)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dataset objects\n# ----------------------\n\n# Training\n# --------\ntrain_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n                                               output_types=(tf.float32, tf.float32),\n                                               \n                                               #1: Batch_size  2:height  3:width  4:channels\n                                               output_shapes=([bs, img_h, img_w, 3], [bs, img_h, img_w, 1]))\ndef prepare_target(x_, y_):\n    y_ = tf.cast(y_, tf.int32)\n    return x_, y_\n\n\n\n#def prepare_target(x_, y_):\n #   y_ = tf.cast(tf.expand_dims(y_[..., 0], -1), tf.int32)\n  #  return x_, tf.where(y_ > 0, y_ - 1, y_ + 1)\n\n#train_dataset = train_dataset.map(prepare_target)\n# Repeat\ntrain_dataset = train_dataset.repeat()\n\n# Validation\n# ----------\nvalid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([bs, img_h, img_w, 3], [bs, img_h, img_w, 1]))\n#valid_dataset = valid_dataset.map(prepare_target)\n\n# Repeat\nvalid_dataset = valid_dataset.repeat()\n\nprint(tf.keras.__version__)\nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RETE UNET N°1** **NOT USED**","metadata":{}},{"cell_type":"code","source":"def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True, activ = True, dropout= False):\n    # first layer\n    x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n               padding=\"same\")(input_tensor) #input_tensor\n    if batchnorm:\n        x = tf.keras.layers.BatchNormalization()(x)\n    if activ:\n        x = tf.keras.layers.Activation(\"relu\")(x)\n    if dropout:\n        x = tf.keras.layers.Dropout(0.2)(x)\n    # second layer\n    x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n               padding=\"same\")(x)\n    #if batchnorm:\n        #x = tf.keras.layers.BatchNormalization()(x)\n   # x = tf.keras.layers.Activation(\"relu\")(x)\n    return x\n\n\n\ndef get_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n    # contracting path\n    inputs = tf.keras.layers.Input((img_h, img_w, 3))\n    s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n    \n    c1 = conv2d_block(s, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n    p1 = tf.keras.layers.MaxPooling2D((2, 2)) (c1)\n    #p1 = tf.keras.layers.Dropout(dropout*0.5)(p1)\n\n    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n    p2 = tf.keras.layers.MaxPooling2D((2, 2)) (c2)\n    #p2 = tf.keras.layers.Dropout(dropout)(p2)\n\n    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n    p3 = tf.keras.layers.MaxPooling2D((2, 2)) (c3)\n   # p3 = tf.keras.layers.Dropout(dropout)(p3)\n    \n  \n    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n   # p4 = tf.keras.layers.Dropout(dropout)(p4)\n  \n    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n    \n    # expansive path\n    print (n_filters)\n    u6 = tf.keras.layers.Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n    u6 = tf.keras.layers.concatenate([u6, c4])\n   # u6 = tf.keras.layers.Dropout(dropout)(u6)\n    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n\n    u7 = tf.keras.layers.Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n    u7 = tf.keras.layers.concatenate([u7, c3])\n    #u7 = tf.keras.layers.Dropout(dropout)(u7)\n    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n\n    u8 = tf.keras.layers.Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n    u8 = tf.keras.layers.concatenate([u8, c2])\n    #u8 = tf.keras.layers.Dropout(dropout)(u8)\n    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=False, activ=False)\n\n    u9 = tf.keras.layers.Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n    #u9 = tf.keras.layers.Dropout(dropout)(u9)\n    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n    \n    c9 = tf.keras.layers.BatchNormalization()(c9)\n    c9 = tf.keras.layers.Activation(\"relu\")(c9)\n    \n    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RETE UNET N°2**","metadata":{}},{"cell_type":"code","source":"# Build U-Net model\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\nfrom keras.layers.merge import concatenate, add\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n    \n    \ndef get_unet2():\n    inputs = tf.keras.layers.Input((img_h, img_w, 3))\n    s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n\n    c1 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(s)\n    c1 = tf.keras.layers.BatchNormalization()(c1)\n    c1 = tf.keras.layers.Activation(\"relu\")(c1)\n    #c1 = tf.keras.layers.Dropout(0.1)(c1)\n    \n    c1 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c1)\n    #c1 = tf.keras.layers.BatchNormalization()(c1)\n    #c1 = tf.keras.layers.Activation(\"relu\")(c1)\n    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\n    c2 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(p1)\n    c2 = tf.keras.layers.BatchNormalization()(c2)\n    c2 = tf.keras.layers.Activation(\"relu\")(c2)\n    #c2 = tf.keras.layers.Dropout(0.1)(c2)\n    \n    \n    c2 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c2)\n    #c2 = tf.keras.layers.BatchNormalization()(c2)\n    #c2 = tf.keras.layers.Activation(\"relu\")(c2)\n    \n    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n\n    c3 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(p2)\n    c3 = tf.keras.layers.BatchNormalization()(c3)\n    c3 = tf.keras.layers.Activation(\"relu\")(c3)\n    #c3 = tf.keras.layers.Dropout(0.2)(c3)\n    \n    c3 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c3)\n    #c3 = tf.keras.layers.BatchNormalization()(c3)\n    #c3 = tf.keras.layers.Activation(\"relu\")(c3)\n    \n    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n\n    c4 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(p3)\n    c4 = tf.keras.layers.BatchNormalization()(c4)\n    c4 = tf.keras.layers.Activation(\"relu\")(c4)\n    #c4 = tf.keras.layers.Dropout(0.2)(c4)\n    \n    c4 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c4)\n    #c4 = tf.keras.layers.BatchNormalization()(c4)\n    #c4 = tf.keras.layers.Activation(\"relu\")(c4)\n    \n    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n\n    c5 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(p4)    \n    c5 = tf.keras.layers.BatchNormalization()(c5)\n    c5 = tf.keras.layers.Activation(\"relu\")(c5)\n    #c5 = tf.keras.layers.Dropout(0.3)(c5)\n    \n    c5 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c5)    \n    #c5 = tf.keras.layers.BatchNormalization()(c5)\n    #c5 = tf.keras.layers.Activation(\"relu\")(c5)\n\n    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = tf.keras.layers.concatenate([u6, c4])\n    c6 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(u6)\n    c6 = tf.keras.layers.BatchNormalization()(c6)\n    c6 = tf.keras.layers.Activation(\"relu\")(c6)\n    #c6 = tf.keras.layers.Dropout(0.2)(c6)\n    c6 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c6)\n    #c6 = tf.keras.layers.BatchNormalization()(c6)\n    #c6 = tf.keras.layers.Activation(\"relu\")(c6)\n\n    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = tf.keras.layers.concatenate([u7, c3])\n    c7 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(u7)\n    c7 = tf.keras.layers.BatchNormalization()(c7)\n    c7 = tf.keras.layers.Activation(\"relu\")(c7)\n    #c7 = tf.keras.layers.Dropout(0.2)(c7)\n    c7 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c7)    \n    #c7 = tf.keras.layers.BatchNormalization()(c7)\n    #c7 = tf.keras.layers.Activation(\"relu\")(c7)\n\n    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = tf.keras.layers.concatenate([u8, c2])\n    c8 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(u8)\n    #c8 = tf.keras.layers.Dropout(0.1)(c8)\n    c8 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c8)    \n    #c8 = tf.keras.layers.BatchNormalization()(c8)\n    #c8 = tf.keras.layers.Activation(\"relu\")(c8)\n\n    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n    c9 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(u9)  \n    c9 = tf.keras.layers.BatchNormalization()(c9)\n    c9 = tf.keras.layers.Activation(\"relu\")(c9)\n    c9 = tf.keras.layers.Dropout(0.2)(c9)\n    \n    c9 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n                                padding='same')(c9)       \n    c9 = tf.keras.layers.BatchNormalization()(c9)\n    c9 = tf.keras.layers.Activation(\"relu\")(c9)\n\n    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n\n    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#input_img = Input((im_height, im_width, 1), name='img')\nmodel = get_unet2()\nmetrics = [my_IoU]\n\nmodel.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    EarlyStopping(patience=10, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model-tgs-salt.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]\n\n#results = model.fit(x = train_dataset, batch_size=4, epochs=30, callbacks=callbacks,\n                    #validation_data=valid_dataset, validation_steps=len(valid_img_gen))\n\n\n\nmodel.fit(x = train_dataset,\n          epochs = 30,\n          steps_per_epoch=len(train_img_gen), \n          validation_data=valid_dataset, \n          validation_steps=len(valid_img_gen),\n          callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compute Prediction**","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\n\n%matplotlib notebook\n\n# Cycle over test images\n\ntest_img_dir = os.path.join(destination_test, 'images', 'img') #test_dir\n\nimg_filenames = next(os.walk(test_img_dir))[2]\n\n\nresults = {}\ni=0\nfor img_filename in img_filenames:\n    \n    i += 1\n    mask_filename = img_filename[:-4] + '.tif' #'.png'\n    \n    img = Image.open(os.path.join(test_img_dir, img_filename))\n    img = img.convert('RGB')\n    img = img.resize((256, 256))\n  \n    \n    \n    img_arr = np.array(img)\n    \n    img_arr = np.expand_dims(np.array(img), 0)\n    \n    pred = model.predict(x=img_arr / 255., batch_size = 8)\n   \n    pred= (pred >0.5).astype(np.uint8)\n    \n\n    # Get predicted class as the index corresponding to the maximum value in the vector probability\n    \n    #predicted_class = tf.argmax(out_softmax, -1)\n    mask = pred[0]\n  \n    \n    \n    mask_pred = rle_encode(mask)\n    mask_name = os.path.splitext(img_filename)[0]\n    results[mask_name] = mask_pred   \n    #if i >4: \n      #  break\n    \n    #time.sleep(1000)\ncreate_csv(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}