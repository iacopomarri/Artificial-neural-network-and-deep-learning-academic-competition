{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \nimport tensorflow as tf\nimport numpy as np\nimport os\nimport shutil\nimport random\nimport json\nfrom PIL import Image\nimport time\nfrom datetime import datetime\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Set the seed for random operations. \n# This let our experiments to be reproducible. \nSEED = 1234\ntf.random.set_seed(SEED)  \n\n# Get current working directory\ncwd = os.getcwd()\n\n# Set GPU memory growth\n# Allows to only as much GPU memory as needed\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Support Functions**","metadata":{}},{"cell_type":"code","source":"#JSON file for validation split, create CSV for results, encode mask, IoU metric\n\n# dictionary with the format shown in the Evaluation tab\ndef create_json():\n    training = os.listdir(os.path.join(destination_training, 'images', 'img'))\n    validation = os.listdir(os.path.join(destination_validation, 'images', 'img'))\n\n    dataset_split = {'training': training, 'validation': validation}   \n    with open('dataset_split.json', 'w') as fp:\n          json.dump(dataset_split, fp)\n        \n        \n\n\n\n#IoU Metric\ndef my_IoU(y_true, y_pred):\n    # from pobability to predicted class {0, 1}\n    y_pred = tf.cast(y_pred > 0.5, tf.float32) # when using sigmoid. Use argmax for softmax\n\n    # A and B\n    intersection = tf.reduce_sum(y_true * y_pred)\n    # A or B\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    # IoU\n    return intersection / union\n\n\n#Encode mask for CSV file\ndef rle_encode(img):\n   \n    img = np.round(np.squeeze(img)).astype(np.int32)\n  \n    #Flatten column-wise\n    pixels = img.T.flatten()\n    pixels = pixels[:-65536]   #provo a togliere la metà dei pixel che viene tutta 1\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\n\n\n\n#Create CSV\ndef create_csv(results, results_dir='./'):\n    \n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    with open(os.path.join('./', csv_fname), 'w') as f:\n        f.write('ImageId,EncodedPixels,Width,Height\\n')\n        count = 0\n        for key, value in results.items():\n            #f.write(key + ',' + str(value) + '\\n')\n            f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ImageDataGenerator\n# ------------------\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\napply_data_augmentation = False\n\n# Create training ImageDataGenerator object\n# We need two different generators for images and corresponding masks\nif apply_data_augmentation:\n    train_img_data_gen = ImageDataGenerator(rotation_range=10,\n                                            width_shift_range=10,\n                                            height_shift_range=10,\n                                            zoom_range=0.3,\n                                            horizontal_flip=True,\n                                            vertical_flip=True,\n                                            fill_mode='constant',\n                                            cval=0,\n                                            rescale=1./255)\n    train_mask_data_gen = ImageDataGenerator(rotation_range=10,\n                                             width_shift_range=10,\n                                             height_shift_range=10,\n                                             zoom_range=0.3,\n                                             horizontal_flip=True,\n                                             vertical_flip=True,\n                                             fill_mode='constant',\n                                             rescale=1./255,\n                                             cval=0)\nelse:\n    train_img_data_gen = ImageDataGenerator(rescale=1./255)\n    train_mask_data_gen = ImageDataGenerator(rescale=1./255)\n\n# Create validation and test ImageDataGenerator objects\nvalid_img_data_gen = ImageDataGenerator(rescale=1./255)\nvalid_mask_data_gen = ImageDataGenerator(rescale=1./255)\ntest_img_data_gen = ImageDataGenerator(rescale=1./255)\ntest_mask_data_gen = ImageDataGenerator(rescale=1./255)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create working directory. Splitting training and validation\n\n\n#Directories\nsource_dir = '/kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset'\ndestination_dir = '/kaggle/working/dataset'\ndestination_training = os.path.join(destination_dir, 'training')\ndestination_validation = os.path.join(destination_dir, 'validation')\ndestination_test = os.path.join(destination_dir, 'test')\n\n\n#Se nella directory di destinazione non ci sono le cartelle dataset, training, validation e test, le creo.\n#Se ci sono già, le elimino e le ricreo (per rifare ogni volta il validation diverso, penso)\n\nprint(\"Creating main directories..\")\nif os.path.exists(destination_dir):\n    shutil.rmtree(destination_dir)\nif not os.path.exists(destination_dir):\n    os.mkdir(destination_dir)\nif not os.path.exists(destination_training):\n    os.mkdir(destination_training)\nif not os.path.exists(destination_validation):\n    os.mkdir(destination_validation)\nif not os.path.exists(destination_test):\n    os.mkdir(destination_test)\n    \n#Create images/img and masks/img folder into training and validation directories. Create img into test directory\n\nprint(\"Creating subdirectories..\")\nif not os.path.exists(os.path.join(destination_training, 'images')):\n    os.mkdir(os.path.join(destination_training, 'images'))\nif not os.path.exists(os.path.join(destination_training, 'masks')):\n    os.mkdir(os.path.join(destination_training, 'masks'))\n    \nif not os.path.exists(os.path.join(destination_training, 'images/img')):\n    os.mkdir(os.path.join(destination_training, 'images/img'))\nif not os.path.exists(os.path.join(destination_training, 'masks/img')):\n    os.mkdir(os.path.join(destination_training, 'masks/img'))\n    \n    \nif not os.path.exists(os.path.join(destination_validation, 'images')):\n    os.mkdir(os.path.join(destination_validation, 'images'))\nif not os.path.exists(os.path.join(destination_validation, 'masks')):\n    os.mkdir(os.path.join(destination_validation, 'masks'))\n    \nif not os.path.exists(os.path.join(destination_validation, 'images/img')):\n    os.mkdir(os.path.join(destination_validation, 'images/img'))\nif not os.path.exists(os.path.join(destination_validation, 'masks/img')):\n    os.mkdir(os.path.join(destination_validation, 'masks/img'))\n    \nif not os.path.exists(os.path.join(destination_test, 'images')):\n    os.mkdir(os.path.join(destination_test, 'images'))\nif not os.path.exists(os.path.join(destination_test, 'images/img')):\n    os.mkdir(os.path.join(destination_test, 'images/img'))\n\n\n\n#Split training_set e validation_set, spostandoli dalla directory di input a quella di lavoro\n\n\n\nprint(\"Splitting train from input into Training and Validation inside working directory...\")\n\n#messi due numeri divisibili per 8. 7 immagini su 7647 vengono però scartate (a random). (valid= circa 15%)\nvalidation_size = 1048 #1528    #int(7647 * validation_split)\ntrain_size = 6592  #6112        #7467 - validation_size\n\nfiles = os.listdir(source_dir + '/training/images/img')\nrandom.shuffle(files)\nfiles = iter(files)\ni=0\n\nwhile i < validation_size:\n    file=str(next(files))\n    result = shutil.copy(source_dir + '/training/images/img' + '/' + file, destination_validation + '/images/img')\n    result = shutil.copy(source_dir + '/training/masks/img' + '/' + file, destination_validation + '/masks/img')\n    i += 1\n\nwhile i < validation_size + train_size:\n    file=str(next(files))\n    result = shutil.copy(source_dir + '/training/images/img' + '/' + file, destination_training + '/images/img')\n    result = shutil.copy(source_dir + '/training/masks/img' + '/' + file, destination_training + '/masks/img')\n    i += 1\n    \n\n        \n\n#Copio i file del test dalla directory di input a quella di lavoro\n\nprint(\"Copying test set from input inside working directory...\")\nfor file in os.listdir(source_dir + '/test/images/img'):\n        result = shutil.copy(source_dir + '/test/images/img' + '/' + file, destination_test + '/images/img')\n       \nprint(\"creating JSON file..\")\ncreate_json()\n\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create generators to read images from dataset directory\n# -------------------------------------------------------\n\n# Batch size\nbs = 4\n\n# img shape\nimg_h = 256\nimg_w = 256\n\nnum_classes=2\n\n# Training\n# Two different generators for images and masks\n# ATTENTION: here the seed is important!! We have to give the same SEED to both the generator\n# to apply the same transformations/shuffling to images and corresponding masks\n\ntrain_img_gen = train_img_data_gen.flow_from_directory(os.path.join(destination_training, 'images'),\n                                                       target_size=(img_h, img_w),\n                                                       batch_size=bs, \n                                                       class_mode=None, # Because we have no class subfolders in this case\n                                                       shuffle=True,\n                                                       interpolation='bilinear',\n                                                       seed=SEED)  \ntrain_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(destination_training, 'masks'),\n                                                         target_size=(img_h, img_w),\n                                                         batch_size=bs,\n                                                         class_mode=None, # Because we have no class subfolders in this case\n                                                         shuffle=True,\n                                                         interpolation='bilinear',\n                                                         color_mode='grayscale',\n                                                         seed=SEED)\ntrain_gen = zip(train_img_gen, train_mask_gen)  #Iteratore che mappa l'i-esimo elemento di train_img_gen con l i-esimo di train_mask_gen\n\n# Validation\nvalid_img_gen = valid_img_data_gen.flow_from_directory(os.path.join(destination_validation, 'images'),\n                                                       target_size=(img_h, img_w),\n                                                       batch_size=bs, \n                                                       class_mode=None, # Because we have no class subfolders in this case\n                                                       shuffle=False,\n                                                       interpolation='bilinear',\n                                                       seed=SEED)\nvalid_mask_gen = valid_mask_data_gen.flow_from_directory(os.path.join(destination_validation, 'masks'),\n                                                         target_size=(img_h, img_w),\n                                                         batch_size=bs, \n                                                         class_mode=None, # Because we have no class subfolders in this case\n                                                         shuffle=False,\n                                                         interpolation='bilinear',\n                                                         color_mode='grayscale',\n                                                         seed=SEED)\nvalid_gen = zip(valid_img_gen, valid_mask_gen)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dataset objects\n# ----------------------\n\n# Training\n# --------\ntrain_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n                                               output_types=(tf.float32, tf.float32),\n                                               \n                                               #1: Batch_size  2:height  3:width  4:channels\n                                               output_shapes=([bs, img_h, img_w, 3], [bs, img_h, img_w, 1]))\n\ndef prepare_target(x_, y_):\n    y_ = tf.cast(tf.expand_dims(y_[..., 0], -1), tf.int32)\n    return x_, tf.where(y_ > 0, y_ - 1, y_ + 1)\n\ntrain_dataset = train_dataset.map(prepare_target)\n# Repeat\ntrain_dataset = train_dataset.repeat()\n\n# Validation\n# ----------\nvalid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([bs, img_h, img_w, 3], [bs, img_h, img_w, 1]))\nvalid_dataset = valid_dataset.map(prepare_target)\n\n# Repeat\nvalid_dataset = valid_dataset.repeat()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.unique(target_img)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convolutional Neural Network (CNN)\n### Encoder-Decoder","metadata":{}},{"cell_type":"code","source":"# Create Model\n# ------------\n\ndef create_model(depth, start_f, num_classes, dynamic_input_shape):\n\n    model = tf.keras.Sequential()\n    \n    # Encoder\n    # -------\n    for i in range(depth):\n        \n        if i == 0:\n            if dynamic_input_shape:\n                input_shape = [None, None, 3]\n            else:\n                input_shape = [img_h, img_w, 3]\n        else:\n            input_shape=[None]\n        \n        model.add(tf.keras.layers.Conv2D(filters=start_f, \n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same',\n                                         input_shape=input_shape))\n        model.add(tf.keras.layers.ReLU())\n        model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n        \n        start_f *= 2\n    \n\n    # Decoder\n    # -------\n    for i in range(depth):\n        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear')) #2 al posto di 1\n        model.add(tf.keras.layers.Conv2D(filters=start_f // 2,\n                                         kernel_size=(3, 3),\n                                         strides=(1, 1),\n                                         padding='same'))\n\n        model.add(tf.keras.layers.ReLU())\n\n        start_f = start_f // 2\n    \n\n    # Prediction Layer\n    # ----------------\n    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n                                     kernel_size=(1, 1),\n                                     strides=(1, 1),\n                                     padding='same',\n                                     activation='sigmoid'))\n    \n    return model       ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(depth=4, \n                     start_f=4, \n                     num_classes=2, \n                     dynamic_input_shape=False)\n\n# Visualize created model as a table\nmodel.summary()\n\n# Visualize initialized weights\nmodel.weights","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the model for training","metadata":{}},{"cell_type":"code","source":"# Optimization params\n# -------------------\n\n# Loss\n# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) \n# learning rate\nlr = 1e-3\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n# -------------------\n\n# Validation metrics\n# ------------------\n\nmetrics = [my_IoU]  #metrics='accuracy'\n# ------------------\n\n# Compile Model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with callbacks","metadata":{}},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\n# from tensorflow.compat.v1 import ConfigProto\n# from tensorflow.compat.v1 import InteractiveSession\n\n# config = ConfigProto()\n# config.gpu_options.allow_growth = True\n# session = InteractiveSession(config=config)\n\ncwd = os.getcwd()\n\nexps_dir = os.path.join(cwd, 'segmentation_experiments')\nif not os.path.exists(exps_dir):\n    os.makedirs(exps_dir)\n\nnow = datetime.now().strftime('%b%d_%H-%M-%S')\n\nmodel_name = 'CNN'\n\nexp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\nif not os.path.exists(exp_dir):\n    os.makedirs(exp_dir)\n    \ncallbacks = []\n\n# Model checkpoint\n# ----------------\nckpt_dir = os.path.join(exp_dir, 'ckpts')\nif not os.path.exists(ckpt_dir):\n    os.makedirs(ckpt_dir)\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n                                                   save_weights_only=True)  # False to save the model directly\ncallbacks.append(ckpt_callback)\n\n# Visualize Learning on Tensorboard\n# ---------------------------------\ntb_dir = os.path.join(exp_dir, 'tb_logs')\nif not os.path.exists(tb_dir):\n    os.makedirs(tb_dir)\n    \n# By default shows losses and metrics for both training and validation\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n                                             profile_batch=0,\n                                             histogram_freq=0)  # if 1 shows weights histograms\ncallbacks.append(tb_callback)\n\n# Early Stopping\n# --------------\nearly_stop = False\nif early_stop:\n    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n    callbacks.append(es_callback)\n\n\nmodel.fit(x=train_dataset,\n          epochs=50,  #### set repeat in training dataset\n          steps_per_epoch=len(train_img_gen),\n          validation_data=valid_dataset,\n          validation_steps=len(valid_img_gen), \n          callbacks=callbacks\n         )\n\n# How to visualize Tensorboard\n\n# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n# 2. localhost:PORT   <- in your browser","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test model","metadata":{}},{"cell_type":"markdown","source":"## Compute prediction","metadata":{}},{"cell_type":"code","source":"import time\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\n\n%matplotlib notebook\n\n# Cycle over test images\n\ntest_img_dir = os.path.join(destination_test, 'images', 'img') #test_dir\n\nimg_filenames = next(os.walk(test_img_dir))[2]\n\n\nresults = {}\ni=0\nfor img_filename in img_filenames:\n    \n    i += 1\n    mask_filename = img_filename[:-4] + '.tif' #'.png'\n    \n    img = Image.open(os.path.join(test_img_dir, img_filename))\n    img = img.convert('RGB')\n    img = img.resize((256, 256))\n  \n    \n    \n    img_arr = np.array(img)\n    \n    img_arr = np.expand_dims(np.array(img), 0)\n    \n    pred = model.predict(x=img_arr / 255., batch_size = 8)\n   \n    pred= (pred >0.5).astype(np.uint8)\n    \n\n    # Get predicted class as the index corresponding to the maximum value in the vector probability\n    \n    #predicted_class = tf.argmax(out_softmax, -1)\n    mask = pred[0]\n  \n    \n    \n    mask_pred = rle_encode(mask)\n    mask_name = os.path.splitext(img_filename)[0]\n    results[mask_name] = mask_pred   \n    #if i >4: \n      #  break\n    \n    #time.sleep(1000)\ncreate_csv(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}